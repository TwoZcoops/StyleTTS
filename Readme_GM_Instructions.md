# StyleTTS Instructions

This is part of Gary Mason's StyleTTS system, Version 1.0.1, built on StyleTTS2 Version 1.0.0.

## Setting up StyleTTS from scratch in VS Code (Windows) using Conda & Jupyter Notebook

This guide provides a comprehensive walkthrough for setting up StyleTTS from scratch in VS Code (Windows) using Conda and Jupyter Notebook, while ensuring Phonemizer and eSpeak NG are correctly configured. This detailed guide will equip you with the knowledge and tools to get started with StyleTTS, a powerful text-to-speech (TTS) model with an impressive ability to generate natural and expressive speech.

## Setting up the Conda Environment

Before we dive into StyleTTS, let's set up our development environment. We'll use Conda, a popular package and environment manager, to create an isolated environment for our project. Using a Conda environment offers several benefits, such as dependency isolation and reproducibility, ensuring a consistent and reliable setup.

Create and activate a new Conda environment in VS Code: Open a terminal in VS Code and run the following command, replacing `ENV_NAME` with your preferred environment name (e.g., styletts-env) and `VERSION` with your desired Python version (e.g., 3.9):

```bash

conda create --name [ENV_NAME] python=[VERSION]

```

If you don't specify a Python version, Conda will install the latest version from the available channels. Activate the newly created environment using the command:

```bash

conda activate <ENV_NAME>

```

You should see the environment name in parentheses at the beginning of your terminal prompt, indicating that the environment is active.

## Installing the Required Packages

With our Conda environment activated, we can now install the necessary packages for StyleTTS.

Install PyTorch, torchaudio, librosa, and Jupyter: These packages are fundamental for audio processing, deep learning tasks, and interactive coding in StyleTTS. Use the following command to install them:

```bash

conda install pytorch torchvision torchaudio -c pytorch librosa jupyter

```

This command installs PyTorch with CUDA support. If you don't have a CUDA-enabled GPU, you can install the CPU-only version of PyTorch.

Install Phonemizer and eSpeak NG: Phonemizer is a library that converts text into phonemes (the basic units of sound in language), and eSpeak NG is a speech synthesizer that we'll use as the backend for Phonemizer. Install them using Conda:

```bash

conda install -c conda-forge phonemizer espeak-ng

```

## Configuring Phonemizer to use eSpeak NG

To ensure StyleTTS works correctly, we need to configure Phonemizer to use eSpeak NG as its backend. This involves setting specific environment variables that point to the eSpeak NG library and executable files.

Set environment variables:

1. Go to the Windows Control Panel and search for "environment variables".
2. Click on "Edit the system environment variables".
3. In the System Properties window, click on "Environment Variables...".
4. Under "System variables", click "New..." to add a new environment variable.
5. For the first variable, set the "Variable name" to `PHONEMIZER_ESPEAK_LIBRARY` and the "Variable value" to the path to the eSpeak NG library file, which is typically `C:\Program Files\eSpeak NG\libespeak-ng.dll`.
6. Click "OK" to save the variable.
7. Repeat the process to add another system variable with the "Variable name" set to `PHONEMIZER_ESPEAK_PATH` and the "Variable value" set to `C:\Program Files\eSpeak NG\espeak-ng.exe`.
8. Click "OK" on all the open windows to close the environment variables settings.

Verify the configuration: To verify that Phonemizer is correctly configured, open a Jupyter Notebook in VS Code and run the following code:

```python
from phonemizer import phonemize

text = "Hello, world!"
phonemes = phonemize(text, language='en-us', backend='espeak')
print(phonemes)
```

If the setup is successful, this code will print the phonemic transcription of "Hello, world!" generated by eSpeak NG.

## Installing StyleTTS

Now that our environment is ready, let's install StyleTTS.

Clone the StyleTTS repository: Clone the StyleTTS repository from GitHub using the following command in your terminal:

```bash

git clone <https://github.com/yl4579/StyleTTS-VC.git>
```

This will download the StyleTTS code to your local machine.

Install additional dependencies: Navigate to the StyleTTS directory using `cd StyleTTS-VC` and install the remaining dependencies using pip:

```bash

pip install -r requirements.txt
```

This command installs all the necessary packages listed in the requirements.txt file.

## Dataset Preparation

Before you can start using StyleTTS, you need to download and prepare the VCTK dataset. This dataset contains speech data that StyleTTS uses for training and inference.

1. Download the VCTK dataset from the official source.
2. Extract the downloaded dataset to a location on your computer.
3. Downsample all the audio files in the `wav48_silence_trimmed` folder to 24 kHz. You can use audio editing software like Audacity to do this.
4. Rename the downsampled folder to `wav24_silence_trimmed`.

This preprocessing is necessary because the vocoder, text aligner, and pitch extractor are pre-trained on 24 kHz data.

## Downloading Pre-trained Models

StyleTTS requires pre-trained models to function. You can download these models from the links provided in the StyleTTS repository. Make sure to download the models that are compatible with your chosen version of StyleTTS. Unzip the downloaded models to the `Models` and `Vocoder` folders within the `StyleTTS-VC` directory.

## Testing the Setup

To test if your StyleTTS installation is working correctly, you can use the following code snippet:

```python
import replicate

# Load environment variables

model_id = "adirik/styletts2:53fd5081feae9440974d1ef9cae83bf7af5fe18be1646343f37e559f5f80a613"

# Text to be synthesized

text = "This is a demo to test StyleTTS."

# Generate speech

output = replicate.run(
    model_id,
    input={"text": text}
)

# Print the output URL

print(output)
```

This code uses the replicate library to interact with the StyleTTS model hosted on Replicate. It takes the text "This is a demo to test StyleTTS" as input and generates an audio file with the synthesized speech. The output of the code is a URL that you can open in your browser to listen to the generated audio.

## Conclusion

By following these steps, you've successfully set up StyleTTS from scratch in VS Code (Windows) using Conda and Jupyter Notebook. You've also configured Phonemizer and eSpeak NG, installed the necessary packages, prepared the dataset, and downloaded the pre-trained models. With this comprehensive setup, you're now ready to explore the capabilities of StyleTTS and generate natural and expressive speech for various applications.
