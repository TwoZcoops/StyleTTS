{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16ca3fd-ca0a-4f24-ac55-a827bba684d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# StyleTTS Demo (LJSpeech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da84c60f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a3ddcc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\Demo\\hifi-gan\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbdc04c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a173af4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_pad = \"$\"\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "dicts = {}\n",
    "for i in range(len((symbols))):\n",
    "    dicts[symbols[i]] = i\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "    def __call__(self, text):\n",
    "        indexes = []\n",
    "        for char in text:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(char)\n",
    "        return indexes\n",
    "\n",
    "textclenaer = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00ee05e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(ref_dicts, model):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "    \n",
    "    return reference_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64fc4c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54cfbe48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported vocoder!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'Vocoder\\g_00750000'\n",
      "Complete.\n",
      "Removing weight norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garym\\AppData\\Local\\Temp\\ipykernel_23492\\3248607396.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(filepath, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# load hifi-gan\n",
    "import sys\n",
    "sys.path.insert(0, \"../Demo/hifi-gan\")\n",
    "sys.path.append(r\"C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\Demo\\hifi-gan\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from attrdict import AttrDict\n",
    "from vocoder import Generator\n",
    "print(\"✅ Successfully imported vocoder!\")\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "h = None\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + '*')\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return ''\n",
    "    return sorted(cp_list)[-1]\n",
    "\n",
    "cp_g = scan_checkpoint(\"Vocoder/\", 'g_')\n",
    "\n",
    "config_file = os.path.join(os.path.split(cp_g)[0], 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "device = torch.device(device)\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(cp_g, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02fb18a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\models.py:667: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(model_path, map_location='cpu')['model']\n",
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\models.py:651: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(path, map_location='cpu')['net']\n",
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "C:\\Users\\garym\\AppData\\Local\\Temp\\ipykernel_23492\\2332377266.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(model_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor loaded\n",
      "decoder loaded\n",
      "pitch_extractor loaded\n",
      "text_encoder loaded\n",
      "style_encoder loaded\n",
      "text_aligner loaded\n"
     ]
    }
   ],
   "source": [
    "# load StyleTTS\n",
    "model_path = \"./Models/LJSpeech/epoch_2nd_00180.pth\"\n",
    "model_config_path = \"./Models/LJSpeech/config.yml\"\n",
    "\n",
    "config = yaml.safe_load(open(model_config_path))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "model = build_model(Munch(config['model_params']), text_aligner, pitch_extractor)\n",
    "\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "params = params['net']\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        if not \"discriminator\" in key:\n",
    "            print('%s loaded' % key)\n",
    "            model[key].load_state_dict(params[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30e8ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garym\\AppData\\Local\\Temp\\ipykernel_23492\\3830952049.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  wave, sr = librosa.load(path, sr=24000)\n",
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LJSpeech-1.1/wavs/LJ050-0234.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'LJSpeech-1.1/wavs/LJ050-0234.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     name \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     ref_dicts[name] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m---> 13\u001b[0m reference_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_style\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 19\u001b[0m, in \u001b[0;36mcompute_style\u001b[1;34m(ref_dicts, model)\u001b[0m\n\u001b[0;32m     17\u001b[0m reference_embeddings \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, path \u001b[38;5;129;01min\u001b[39;00m ref_dicts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 19\u001b[0m     wave, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     audio, index \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(wave, top_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m24000\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LJSpeech-1.1/wavs/LJ050-0234.wav'"
     ]
    }
   ],
   "source": [
    "# get first 3 training sample as references\n",
    "\n",
    "train_path = config.get('train_data', None)\n",
    "val_path = config.get('val_data', None)\n",
    "train_list, val_list = get_data_path_list(train_path, val_path)\n",
    "\n",
    "ref_dicts = {}\n",
    "for j in range(3):\n",
    "    filename = train_list[j].split('|')[0]\n",
    "    name = filename.split('/')[-1].replace('.wav', '')\n",
    "    ref_dicts[name] = filename\n",
    "    \n",
    "reference_embeddings = compute_style(ref_dicts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24655f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize a text\n",
    "text = ''' StyleTTS is a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens.append(0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_samples = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "    m = length_to_mask(input_lengths).to(device)\n",
    "    t_en = model.text_encoder(tokens, input_lengths, m)\n",
    "        \n",
    "    for key, (ref, _) in reference_embeddings.items():\n",
    "        \n",
    "        s = ref.squeeze(1)\n",
    "        style = s\n",
    "        \n",
    "        d = model.predictor.text_encoder(t_en, style, input_lengths, m)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "        \n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        style = s.expand(en.shape[0], en.shape[1], -1)\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)), \n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze().cpu().numpy()\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze()\n",
    "        \n",
    "        converted_samples[key] = y_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7f7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "for key, wave in converted_samples.items():\n",
    "    print('Synthesized: %s' % key)\n",
    "    display(ipd.Audio(wave, rate=24000))\n",
    "    try:\n",
    "        print('Reference: %s' % key)\n",
    "        display(ipd.Audio(reference_embeddings[key][-1], rate=24000))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe14d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
