{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16ca3fd-ca0a-4f24-ac55-a827bba684d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# StyleTTS Demo (LJSpeech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a400d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\n",
      "Python Path: ['c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\workspaces\\\\StyleTTS', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\python39.zip', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\DLLs', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts', '', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory explicitly\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Print to verify Python sees the correct directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"Python Path:\", sys.path)\n",
    "\n",
    "# Now import\n",
    "from models import *  \n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958a9f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful! All modules are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup for garo_Inference_LJSpeech.ipynb\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.getcwd())  # Ensure the working directory is first in sys.path\n",
    "\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "from models import *  # This should now work correctly\n",
    "from utils import *\n",
    "# %matplotlib inline # Keep or remove as needed\n",
    "print(\"✅ Imports successful! All modules are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3ddcc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ eSpeak is properly detected by Phonemizer!\n"
     ]
    }
   ],
   "source": [
    "# This is mostly for verification, not setup.  The environment\n",
    "# variables should be set *before* launching the notebook.\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "try:\n",
    "    backend = EspeakBackend(\"en-us\")\n",
    "    print(\"✅ eSpeak is properly detected by Phonemizer!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ eSpeak detection failed!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdc04c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a173af4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text Cleaner is working! Processed Output: [24, 47, 54, 54, 57, 3, 16, 65, 57, 60, 54, 46, 5]\n"
     ]
    }
   ],
   "source": [
    "_pad = \"$\"\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "dicts = {symbols[i]: i for i in range(len(symbols))}\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "\n",
    "    def __call__(self, text):\n",
    "        indexes = []\n",
    "        for char in text:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(f\"⚠️ Warning: Character '{char}' not found in dictionary!\")\n",
    "        return indexes\n",
    "\n",
    "# Initialize the text cleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "# ✅ Test it immediately\n",
    "test_text = \"Hello, world!\"\n",
    "cleaned_text = textclenaer(test_text)\n",
    "\n",
    "print(\"✅ Text Cleaner is working! Processed Output:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ee05e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing test successful! Mel Shape: torch.Size([1, 80, 81])\n"
     ]
    }
   ],
   "source": [
    "# Define Mel Spectrogram transformation\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "# Function to create a mask for padding\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "# Preprocess waveform into Mel spectrogram\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "# Compute style embeddings for reference audio files\n",
    "def compute_style(ref_dicts, model):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "    \n",
    "    return reference_embeddings\n",
    "\n",
    "# ✅ Test the preprocess function with random noise\n",
    "test_wave = np.random.randn(24000)  # 1 second of fake audio\n",
    "mel_output = preprocess(test_wave)\n",
    "\n",
    "print(\"✅ Preprocessing test successful! Mel Shape:\", mel_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64fc4c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phonemizer test successful! Output: ['həlˈoʊ, wˈɜːld! ']\n"
     ]
    }
   ],
   "source": [
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
    "test_text = \"Hello, world!\"\n",
    "phonemes = global_phonemizer.phonemize([test_text])\n",
    "\n",
    "print(\"✅ Phonemizer test successful! Output:\", phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cfbe48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported vocoder!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'Vocoder\\g_00750000'\n",
      "Complete.\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# load hifi-gan\n",
    "import sys\n",
    "sys.path.insert(0, \"../Demo/hifi-gan\")\n",
    "sys.path.append(r\"C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\Demo\\hifi-gan\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from attrdict import AttrDict\n",
    "from vocoder import Generator\n",
    "print(\"✅ Successfully imported vocoder!\")\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "h = None\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device, weights_only=True)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + '*')\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return ''\n",
    "    return sorted(cp_list)[-1]\n",
    "\n",
    "cp_g = scan_checkpoint(\"Vocoder/\", 'g_')\n",
    "\n",
    "config_file = os.path.join(os.path.split(cp_g)[0], 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "device = torch.device(device)\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(cp_g, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02fb18a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hidden_dim', 'n_token', 'style_dim', 'n_layer', 'dim_in', 'max_conv_dim', 'n_mels', 'dropout'])\n",
      "⚠️ Warning: 'decoder' is missing in config! Using default settings.\n",
      "⚠️ Warning: 'max_dur' is missing in config! Using default value.\n",
      "⚠️ Warning: 'multispeaker' is missing in config! Using default value: False.\n",
      "⚠️ Warning: 'diffusion' is missing in config! Using minimal default.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 4 required positional arguments: 'num_layers', 'num_heads', 'head_features', and 'multiplier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 55\u001b[0m\n\u001b[0;32m     49\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_params\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiffusion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m Munch({\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: Munch({}) \u001b[38;5;66;03m# Minimal transformer config, may need more params\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     })\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Now call build_model() with the correct number of arguments\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_aligner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpitch_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Load model parameters\u001b[39;00m\n\u001b[0;32m     58\u001b[0m params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\models.py:649\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(args, text_aligner, pitch_extractor, bert)\u001b[0m\n\u001b[0;32m    644\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m StyleTransformer1d(channels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstyle_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m    645\u001b[0m                                 context_embedding_features\u001b[38;5;241m=\u001b[39mbert\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    646\u001b[0m                                 context_features\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstyle_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m    647\u001b[0m                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mtransformer)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     transformer \u001b[38;5;241m=\u001b[39m Transformer1d(channels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstyle_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m    650\u001b[0m                                 context_embedding_features\u001b[38;5;241m=\u001b[39mbert\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    651\u001b[0m                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mtransformer)\n\u001b[0;32m    653\u001b[0m diffusion \u001b[38;5;241m=\u001b[39m AudioDiffusionConditional(\n\u001b[0;32m    654\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    655\u001b[0m     embedding_max_length\u001b[38;5;241m=\u001b[39mbert\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    659\u001b[0m     context_features\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstyle_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    662\u001b[0m diffusion\u001b[38;5;241m.\u001b[39mdiffusion \u001b[38;5;241m=\u001b[39m KDiffusion(\n\u001b[0;32m    663\u001b[0m     net\u001b[38;5;241m=\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39munet,\n\u001b[0;32m    664\u001b[0m     sigma_distribution\u001b[38;5;241m=\u001b[39mLogNormalDistribution(mean \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39mmean, std \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39mstd),\n\u001b[0;32m    665\u001b[0m     sigma_data\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39msigma_data, \u001b[38;5;66;03m# a placeholder, will be changed dynamically when start training diffusion model\u001b[39;00m\n\u001b[0;32m    666\u001b[0m     dynamic_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \n\u001b[0;32m    667\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 4 required positional arguments: 'num_layers', 'num_heads', 'head_features', and 'multiplier'"
     ]
    }
   ],
   "source": [
    "# Load StyleTTS\n",
    "model_path = \"./Models/LJSpeech/epoch_2nd_00180.pth\"\n",
    "model_config_path = \"./Models/LJSpeech/config.yml\"\n",
    "\n",
    "config = yaml.safe_load(open(model_config_path))\n",
    "\n",
    "# Load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# Load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# ✅ Fix: Load a default BERT model since it's missing in config.yml\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"  # Default BERT model\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
    "bert = AutoModel.from_pretrained(bert_model_name, config=bert_config)\n",
    "print(config['model_params'].keys())  # Check which keys exist\n",
    "if 'decoder' not in config['model_params']:\n",
    "    print(\"⚠️ Warning: 'decoder' is missing in config! Using default settings.\")\n",
    "    config['model_params']['decoder'] = Munch({  # ✅ Convert it to Munch here\n",
    "        'type': 'hifigan',  # Default decoder type\n",
    "        'resblock_kernel_sizes': [3, 7, 11],\n",
    "        'upsample_rates': [8, 8, 2, 2],\n",
    "        'upsample_initial_channel': 512,\n",
    "        'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "        'upsample_kernel_sizes': [16, 16, 4, 4],\n",
    "        'gen_istft_n_fft': 1024,\n",
    "        'gen_istft_hop_size': 256\n",
    "    })\n",
    "\n",
    "# ✅ Add default max_dur if it's missing\n",
    "if 'max_dur' not in config['model_params']:\n",
    "    print(\"⚠️ Warning: 'max_dur' is missing in config! Using default value.\")\n",
    "    config['model_params']['max_dur'] = 100 # You can adjust this value if needed\n",
    "\n",
    "# ✅ Add default multispeaker if it's missing\n",
    "if 'multispeaker' not in config['model_params']:\n",
    "    print(\"⚠️ Warning: 'multispeaker' is missing in config! Using default value: False.\")\n",
    "    config['model_params']['multispeaker'] = False # Default to single speaker model\n",
    "\n",
    "# ✅ Add default diffusion config - minimal, might need adjustment\n",
    "if 'diffusion' not in config['model_params']:\n",
    "    print(\"⚠️ Warning: 'diffusion' is missing in config! Using minimal default.\")\n",
    "    config['model_params']['diffusion'] = Munch({\n",
    "        'transformer': Munch({}) # Minimal transformer config, may need more params\n",
    "    })\n",
    "\n",
    "\n",
    "# Now call build_model() with the correct number of arguments\n",
    "model = build_model(Munch(config['model_params']), text_aligner, pitch_extractor, bert)\n",
    "\n",
    "# Load model parameters\n",
    "params = torch.load(model_path, map_location='cpu')\n",
    "params = params['net']\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        if not \"discriminator\" in key:\n",
    "            print('%s loaded' % key)\n",
    "            model[key].load_state_dict(params[key])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 3 training sample as references\n",
    "\n",
    "train_path = config.get('train_data', None)\n",
    "val_path = config.get('val_data', None)\n",
    "train_list, val_list = get_data_path_list(train_path, val_path)\n",
    "\n",
    "ref_dicts = {}\n",
    "for j in range(3):\n",
    "    filename = train_list[j].split('|')[0]\n",
    "    name = filename.split('/')[-1].replace('.wav', '')\n",
    "    ref_dicts[name] = filename\n",
    "    \n",
    "reference_embeddings = compute_style(ref_dicts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24655f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize a text\n",
    "text = ''' StyleTTS is a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens.append(0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_samples = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "    m = length_to_mask(input_lengths).to(device)\n",
    "    t_en = model.text_encoder(tokens, input_lengths, m)\n",
    "        \n",
    "    for key, (ref, _) in reference_embeddings.items():\n",
    "        \n",
    "        s = ref.squeeze(1)\n",
    "        style = s\n",
    "        \n",
    "        d = model.predictor.text_encoder(t_en, style, input_lengths, m)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "        \n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        style = s.expand(en.shape[0], en.shape[1], -1)\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)), \n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze().cpu().numpy()\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze()\n",
    "        \n",
    "        converted_samples[key] = y_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7f7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "for key, wave in converted_samples.items():\n",
    "    print('Synthesized: %s' % key)\n",
    "    display(ipd.Audio(wave, rate=24000))\n",
    "    try:\n",
    "        print('Reference: %s' % key)\n",
    "        display(ipd.Audio(reference_embeddings[key][-1], rate=24000))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe14d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styletts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
