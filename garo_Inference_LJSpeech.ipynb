{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16ca3fd-ca0a-4f24-ac55-a827bba684d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# StyleTTS Demo (LJSpeech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108384d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a400d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\n",
      "Python Path: ['c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\workspaces\\\\StyleTTS', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\python39.zip', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\DLLs', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts', '', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\garym\\\\OneDrive\\\\Scripts\\\\GM_Alienware\\\\opt\\\\Conda\\\\envs\\\\styletts\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory explicitly\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Print to verify Python sees the correct directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"Python Path:\", sys.path)\n",
    "\n",
    "# Now import\n",
    "from models import *  \n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958a9f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful! All modules are loaded.\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup for garo_Inference_LJSpeech.ipynb\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.getcwd())  # Ensure the working directory is first in sys.path\n",
    "\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "from models import *  # This should now work correctly\n",
    "from utils import *\n",
    "# %matplotlib inline # Keep or remove as needed\n",
    "print(\"✅ Imports successful! All modules are loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3ddcc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ eSpeak is properly detected by Phonemizer!\n"
     ]
    }
   ],
   "source": [
    "# This is mostly for verification, not setup.  The environment\n",
    "# variables should be set *before* launching the notebook.\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "try:\n",
    "    backend = EspeakBackend(\"en-us\")\n",
    "    print(\"✅ eSpeak is properly detected by Phonemizer!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ eSpeak detection failed!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdc04c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a173af4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text Cleaner is working! Processed Output: [24, 47, 54, 54, 57, 3, 16, 65, 57, 60, 54, 46, 5]\n"
     ]
    }
   ],
   "source": [
    "_pad = \"$\"\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“” '\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "# Export all symbols:\n",
    "symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "dicts = {symbols[i]: i for i in range(len(symbols))}\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self, dummy=None):\n",
    "        self.word_index_dictionary = dicts\n",
    "\n",
    "    def __call__(self, text):\n",
    "        indexes = []\n",
    "        for char in text:\n",
    "            try:\n",
    "                indexes.append(self.word_index_dictionary[char])\n",
    "            except KeyError:\n",
    "                print(f\"⚠️ Warning: Character '{char}' not found in dictionary!\")\n",
    "        return indexes\n",
    "\n",
    "# Initialize the text cleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "# ✅ Test it immediately\n",
    "test_text = \"Hello, world!\"\n",
    "cleaned_text = textclenaer(test_text)\n",
    "\n",
    "print(\"✅ Text Cleaner is working! Processed Output:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ee05e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing test successful! Mel Shape: torch.Size([1, 80, 81])\n"
     ]
    }
   ],
   "source": [
    "# Define Mel Spectrogram transformation\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "# Function to create a mask for padding\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "# Preprocess waveform into Mel spectrogram\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "# Compute style embeddings for reference audio files\n",
    "def compute_style(ref_dicts, model):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "    \n",
    "    return reference_embeddings\n",
    "\n",
    "# ✅ Test the preprocess function with random noise\n",
    "test_wave = np.random.randn(24000)  # 1 second of fake audio\n",
    "mel_output = preprocess(test_wave)\n",
    "\n",
    "print(\"✅ Preprocessing test successful! Mel Shape:\", mel_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cecbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64fc4c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phonemizer test successful! Output: ['həlˈoʊ, wˈɜːld! ']\n"
     ]
    }
   ],
   "source": [
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
    "test_text = \"Hello, world!\"\n",
    "phonemes = global_phonemizer.phonemize([test_text])\n",
    "\n",
    "print(\"✅ Phonemizer test successful! Output:\", phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cfbe48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported vocoder!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\opt\\Conda\\envs\\styletts\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'Vocoder\\g_00750000'\n",
      "Complete.\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# load hifi-gan\n",
    "import sys\n",
    "sys.path.insert(0, \"../Demo/hifi-gan\")\n",
    "sys.path.append(r\"C:\\Users\\garym\\OneDrive\\Scripts\\GM_Alienware\\workspaces\\StyleTTS\\Demo\\hifi-gan\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from attrdict import AttrDict\n",
    "from vocoder import Generator\n",
    "print(\"✅ Successfully imported vocoder!\")\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "\n",
    "h = None\n",
    "\n",
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device, weights_only=True)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict\n",
    "\n",
    "def scan_checkpoint(cp_dir, prefix):\n",
    "    pattern = os.path.join(cp_dir, prefix + '*')\n",
    "    cp_list = glob.glob(pattern)\n",
    "    if len(cp_list) == 0:\n",
    "        return ''\n",
    "    return sorted(cp_list)[-1]\n",
    "\n",
    "cp_g = scan_checkpoint(\"Vocoder/\", 'g_')\n",
    "\n",
    "config_file = os.path.join(os.path.split(cp_g)[0], 'config.json')\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "device = torch.device(device)\n",
    "generator = Generator(h).to(device)\n",
    "\n",
    "state_dict_g = load_checkpoint(cp_g, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02fb18a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['hidden_dim', 'n_token', 'style_dim', 'n_layer', 'dim_in', 'max_conv_dim', 'n_mels', 'dropout', 'decoder'])\n",
      "⚠️ Warning: 'max_dur' is missing in config! Using default value: 1 (to match pretrained model).\n",
      "⚠️ Warning: 'multispeaker' is missing in config! Using default value: False.\n",
      "⚠️ Warning: 'diffusion' is missing in config! Using default with transformer and embedding_mask_proba.\n",
      "⚠️ Warning: 'slm' is missing in config! Using default values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garym\\AppData\\Local\\Temp\\ipykernel_17516\\3861486645.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  params = torch.load(model_path, map_location='cpu')['net']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor loaded\n",
      "decoder loaded\n",
      "Skipping parameter 'decode.2.conv1.bias' due to shape mismatch: checkpoint shape torch.Size([512]), model shape torch.Size([1024])\n",
      "Skipping parameter 'decode.2.conv1.weight_g' due to shape mismatch: checkpoint shape torch.Size([512, 1, 1]), model shape torch.Size([1024, 1, 1])\n",
      "Skipping parameter 'decode.2.conv1.weight_v' due to shape mismatch: checkpoint shape torch.Size([512, 1090, 3]), model shape torch.Size([1024, 1090, 3])\n",
      "Skipping parameter 'decode.2.conv2.bias' due to shape mismatch: checkpoint shape torch.Size([512]), model shape torch.Size([1024])\n",
      "Skipping parameter 'decode.2.conv2.weight_g' due to shape mismatch: checkpoint shape torch.Size([512, 1, 1]), model shape torch.Size([1024, 1, 1])\n",
      "Skipping parameter 'decode.2.conv2.weight_v' due to shape mismatch: checkpoint shape torch.Size([512, 512, 3]), model shape torch.Size([1024, 1024, 3])\n",
      "Skipping parameter 'decode.2.norm2.fc.weight' due to shape mismatch: checkpoint shape torch.Size([1024, 128]), model shape torch.Size([2048, 128])\n",
      "Skipping parameter 'decode.2.norm2.fc.bias' due to shape mismatch: checkpoint shape torch.Size([1024]), model shape torch.Size([2048])\n",
      "Skipping parameter 'decode.2.conv1x1.weight_g' due to shape mismatch: checkpoint shape torch.Size([512, 1, 1]), model shape torch.Size([1024, 1, 1])\n",
      "Skipping parameter 'decode.2.conv1x1.weight_v' due to shape mismatch: checkpoint shape torch.Size([512, 1090, 1]), model shape torch.Size([1024, 1090, 1])\n",
      "Parameter 'decode.2.pool.bias' not found in model; skipping.\n",
      "Parameter 'decode.2.pool.weight_g' not found in model; skipping.\n",
      "Parameter 'decode.2.pool.weight_v' not found in model; skipping.\n",
      "Skipping parameter 'decode.3.conv1.weight_v' due to shape mismatch: checkpoint shape torch.Size([512, 512, 3]), model shape torch.Size([512, 1090, 3])\n",
      "Skipping parameter 'decode.3.norm1.fc.weight' due to shape mismatch: checkpoint shape torch.Size([1024, 128]), model shape torch.Size([2180, 128])\n",
      "Skipping parameter 'decode.3.norm1.fc.bias' due to shape mismatch: checkpoint shape torch.Size([1024]), model shape torch.Size([2180])\n",
      "Parameter 'decode.4.conv1.bias' not found in model; skipping.\n",
      "Parameter 'decode.4.conv1.weight_g' not found in model; skipping.\n",
      "Parameter 'decode.4.conv1.weight_v' not found in model; skipping.\n",
      "Parameter 'decode.4.conv2.bias' not found in model; skipping.\n",
      "Parameter 'decode.4.conv2.weight_g' not found in model; skipping.\n",
      "Parameter 'decode.4.conv2.weight_v' not found in model; skipping.\n",
      "Parameter 'decode.4.norm1.fc.weight' not found in model; skipping.\n",
      "Parameter 'decode.4.norm1.fc.bias' not found in model; skipping.\n",
      "Parameter 'decode.4.norm2.fc.weight' not found in model; skipping.\n",
      "Parameter 'decode.4.norm2.fc.bias' not found in model; skipping.\n",
      "Parameter 'encode.0.conv1.bias' not found in model; skipping.\n",
      "Parameter 'encode.0.conv1.weight_g' not found in model; skipping.\n",
      "Parameter 'encode.0.conv1.weight_v' not found in model; skipping.\n",
      "Parameter 'encode.0.conv2.bias' not found in model; skipping.\n",
      "Parameter 'encode.0.conv2.weight_g' not found in model; skipping.\n",
      "Parameter 'encode.0.conv2.weight_v' not found in model; skipping.\n",
      "Parameter 'encode.0.norm1.weight' not found in model; skipping.\n",
      "Parameter 'encode.0.norm1.bias' not found in model; skipping.\n",
      "Parameter 'encode.0.norm2.weight' not found in model; skipping.\n",
      "Parameter 'encode.0.norm2.bias' not found in model; skipping.\n",
      "Parameter 'encode.0.conv1x1.weight_g' not found in model; skipping.\n",
      "Parameter 'encode.0.conv1x1.weight_v' not found in model; skipping.\n",
      "Parameter 'encode.1.conv1.bias' not found in model; skipping.\n",
      "Parameter 'encode.1.conv1.weight_g' not found in model; skipping.\n",
      "Parameter 'encode.1.conv1.weight_v' not found in model; skipping.\n",
      "Parameter 'encode.1.conv2.bias' not found in model; skipping.\n",
      "Parameter 'encode.1.conv2.weight_g' not found in model; skipping.\n",
      "Parameter 'encode.1.conv2.weight_v' not found in model; skipping.\n",
      "Parameter 'encode.1.norm1.weight' not found in model; skipping.\n",
      "Parameter 'encode.1.norm1.bias' not found in model; skipping.\n",
      "Parameter 'encode.1.norm2.weight' not found in model; skipping.\n",
      "Parameter 'encode.1.norm2.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv1.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv1.weight_g' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv1.weight_v' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv2.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv2.weight_g' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv2.weight_v' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.norm1.weight' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.norm1.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.norm2.weight' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.norm2.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv1x1.weight_g' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.conv1x1.weight_v' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.pool.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.pool.weight_g' not found in model; skipping.\n",
      "Parameter 'F0_conv.0.pool.weight_v' not found in model; skipping.\n",
      "Parameter 'F0_conv.1.bias' not found in model; skipping.\n",
      "Parameter 'F0_conv.1.weight_g' not found in model; skipping.\n",
      "Parameter 'F0_conv.1.weight_v' not found in model; skipping.\n",
      "Parameter 'F0_conv.2.weight' not found in model; skipping.\n",
      "Parameter 'F0_conv.2.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv1.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv1.weight_g' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv1.weight_v' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv2.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv2.weight_g' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv2.weight_v' not found in model; skipping.\n",
      "Parameter 'N_conv.0.norm1.weight' not found in model; skipping.\n",
      "Parameter 'N_conv.0.norm1.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.norm2.weight' not found in model; skipping.\n",
      "Parameter 'N_conv.0.norm2.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv1x1.weight_g' not found in model; skipping.\n",
      "Parameter 'N_conv.0.conv1x1.weight_v' not found in model; skipping.\n",
      "Parameter 'N_conv.0.pool.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.0.pool.weight_g' not found in model; skipping.\n",
      "Parameter 'N_conv.0.pool.weight_v' not found in model; skipping.\n",
      "Parameter 'N_conv.1.bias' not found in model; skipping.\n",
      "Parameter 'N_conv.1.weight_g' not found in model; skipping.\n",
      "Parameter 'N_conv.1.weight_v' not found in model; skipping.\n",
      "Parameter 'N_conv.2.weight' not found in model; skipping.\n",
      "Parameter 'N_conv.2.bias' not found in model; skipping.\n",
      "Parameter 'asr_res.1.weight' not found in model; skipping.\n",
      "Parameter 'asr_res.1.bias' not found in model; skipping.\n",
      "Parameter 'to_out.0.bias' not found in model; skipping.\n",
      "Parameter 'to_out.0.weight_g' not found in model; skipping.\n",
      "Parameter 'to_out.0.weight_v' not found in model; skipping.\n",
      "text_encoder loaded\n",
      "style_encoder loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n"
     ]
    }
   ],
   "source": [
    "from munch import munchify  # Recursively converts dicts to Munch objects\n",
    "\n",
    "def load_partial_state_dict(module, state_dict):\n",
    "    \"\"\"\n",
    "    Loads parameters from state_dict into module for those keys that match in shape.\n",
    "    \"\"\"\n",
    "    model_dict = module.state_dict()\n",
    "    filtered_dict = {}\n",
    "    for name, param in state_dict.items():\n",
    "        if name in model_dict:\n",
    "            if param.shape == model_dict[name].shape:\n",
    "                filtered_dict[name] = param\n",
    "            else:\n",
    "                print(f\"Skipping parameter '{name}' due to shape mismatch: \"\n",
    "                      f\"checkpoint shape {param.shape}, model shape {model_dict[name].shape}\")\n",
    "        else:\n",
    "            print(f\"Parameter '{name}' not found in model; skipping.\")\n",
    "    # Update the module's state dict with the filtered values and load with strict=False\n",
    "    model_dict.update(filtered_dict)\n",
    "    module.load_state_dict(model_dict, strict=False)\n",
    "    return\n",
    "\n",
    "# Load StyleTTS\n",
    "model_path = \"./Models/LJSpeech/epoch_2nd_00180.pth\"\n",
    "model_config_path = \"./Models/LJSpeech/config.yml\"\n",
    "\n",
    "# Recursively convert the configuration dict to a Munch object\n",
    "config = munchify(yaml.safe_load(open(model_config_path)))\n",
    "\n",
    "# Load pretrained ASR model\n",
    "text_aligner = load_ASR_models(config.ASR_path, config.ASR_config)\n",
    "\n",
    "# Load pretrained F0 model\n",
    "pitch_extractor = load_F0_models(config.F0_path)\n",
    "\n",
    "# ✅ Fix: Load a default BERT model since it's missing in config.yml\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"  # Default BERT model\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
    "bert = AutoModel.from_pretrained(bert_model_name, config=bert_config)\n",
    "print(config.model_params.keys())  # Check which keys exist\n",
    "\n",
    "# Ensure the necessary keys exist in model_params (they are now Munch objects)\n",
    "if not hasattr(config.model_params, 'decoder'):\n",
    "    print(\"⚠️ Warning: 'decoder' is missing in config! Using default settings.\")\n",
    "    config.model_params.decoder = munchify({\n",
    "        'type': 'hifigan',\n",
    "        'resblock_kernel_sizes': [3, 7, 11],\n",
    "        'upsample_rates': [10, 5, 3, 2],\n",
    "        'upsample_initial_channel': 512,\n",
    "        'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "        'upsample_kernel_sizes': [20, 10, 6, 4]\n",
    "    })\n",
    "\n",
    "if not hasattr(config.model_params, 'max_dur'):\n",
    "    print(\"⚠️ Warning: 'max_dur' is missing in config! Using default value: 1 (to match pretrained model).\")\n",
    "    config.model_params.max_dur = 1\n",
    "\n",
    "if not hasattr(config.model_params, 'multispeaker'):\n",
    "    print(\"⚠️ Warning: 'multispeaker' is missing in config! Using default value: False.\")\n",
    "    config.model_params.multispeaker = False\n",
    "\n",
    "if not hasattr(config.model_params, 'diffusion'):\n",
    "    print(\"⚠️ Warning: 'diffusion' is missing in config! Using default with transformer and embedding_mask_proba.\")\n",
    "    config.model_params.diffusion = munchify({\n",
    "        'transformer': {\n",
    "            'num_layers': 2,\n",
    "            'num_heads': 8,\n",
    "            'head_features': 64,\n",
    "            'multiplier': 4\n",
    "        },\n",
    "        'dist': {\n",
    "            'mean': 0.0,\n",
    "            'std': 1.0,\n",
    "            'sigma_data': 0.5\n",
    "        },\n",
    "        'embedding_mask_proba': 0.1\n",
    "    })\n",
    "\n",
    "if not hasattr(config.model_params, 'slm'):\n",
    "    print(\"⚠️ Warning: 'slm' is missing in config! Using default values.\")\n",
    "    config.model_params.slm = munchify({\n",
    "        'hidden': 256,\n",
    "        'nlayers': 2,\n",
    "        'initial_channel': 512\n",
    "    })\n",
    "\n",
    "# Now call build_model() with the proper configuration\n",
    "model = build_model(config.model_params, text_aligner, pitch_extractor, bert)\n",
    "\n",
    "# Load model parameters from checkpoint\n",
    "params = torch.load(model_path, map_location='cpu')['net']\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        if \"discriminator\" not in key:\n",
    "            print(f\"{key} loaded\")\n",
    "            load_partial_state_dict(model[key], params[key])\n",
    "\n",
    "# Set model to evaluation mode and move to device\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803110e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 3 training sample as references\n",
    "\n",
    "train_path = config.get('train_data', None)\n",
    "val_path = config.get('val_data', None)\n",
    "train_list, val_list = get_data_path_list(train_path, val_path)\n",
    "\n",
    "ref_dicts = {}\n",
    "for j in range(3):\n",
    "    filename = train_list[j].split('|')[0]\n",
    "    name = filename.split('/')[-1].replace('.wav', '')\n",
    "    ref_dicts[name] = filename\n",
    "    \n",
    "reference_embeddings = compute_style(ref_dicts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24655f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize a text\n",
    "text = ''' StyleTTS is a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = ' '.join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens.append(0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_samples = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "    m = length_to_mask(input_lengths).to(device)\n",
    "    t_en = model.text_encoder(tokens, input_lengths, m)\n",
    "        \n",
    "    for key, (ref, _) in reference_embeddings.items():\n",
    "        \n",
    "        s = ref.squeeze(1)\n",
    "        style = s\n",
    "        \n",
    "        d = model.predictor.text_encoder(t_en, style, input_lengths, m)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "        \n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        style = s.expand(en.shape[0], en.shape[1], -1)\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)), \n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze().cpu().numpy()\n",
    "\n",
    "        c = out.squeeze()\n",
    "        y_g_hat = generator(c.unsqueeze(0))\n",
    "        y_out = y_g_hat.squeeze()\n",
    "        \n",
    "        converted_samples[key] = y_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7f7d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "for key, wave in converted_samples.items():\n",
    "    print('Synthesized: %s' % key)\n",
    "    display(ipd.Audio(wave, rate=24000))\n",
    "    try:\n",
    "        print('Reference: %s' % key)\n",
    "        display(ipd.Audio(reference_embeddings[key][-1], rate=24000))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe14d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styletts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
